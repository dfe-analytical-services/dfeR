% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/write_df_to_delta_utils.R
\name{copy_into_delta}
\alias{copy_into_delta}
\title{Load Parquet Data into a Delta Table using COPY INTO}
\usage{
copy_into_delta(
  target_table,
  volume_file,
  db_conn,
  copy_options = "'mergeSchema' = 'true'"
)
}
\arguments{
\item{target_table}{A character string specifying the name of the Delta
table. Can be unqualified (\code{"table"}), partially qualified
(\code{"schema.table"}), or fully qualified (\code{"catalog.schema.table"}).
If not fully qualified, the function resolves the catalog and/or schema from
the active database connection.}

\item{volume_file}{A character string with the full path to the Parquet file
in a Databricks Volume (e.g., \verb{/Volumes/.../data.parquet}).}

\item{db_conn}{A DBI database connection object to Databricks.}

\item{copy_options}{A character string of key-value pairs to customise the
\verb{COPY INTO} behaviour. Defaults to \code{'mergeSchema' = 'true'}.}
}
\value{
The result of executing the \verb{COPY INTO} SQL command.
}
\description{
This function executes a \verb{COPY INTO} SQL command to load data from a Parquet
file in a Databricks Volume into a Delta Lake table.
}
\examples{
\dontrun{
con <- DBI::dbConnect(odbc::databricks(),
  httpPath = Sys.getenv("DATABRICKS_SQL_PATH")
)
copy_into_delta(
  "catalog.schema.my_table",
  "/Volumes/my_volume/data_file.parquet",
  con
)
}

}
\keyword{internal}
