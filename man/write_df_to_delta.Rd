% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/write_df_to_delta.R
\name{write_df_to_delta}
\alias{write_df_to_delta}
\title{Write a Data Frame to Delta Lake with COPY INTO}
\usage{
write_df_to_delta(
  df,
  target_table,
  db_conn,
  column_types_schema = NULL,
  volume_dir,
  copy_options = "'mergeSchema' = 'true'",
  overwrite_table = FALSE,
  chunk_size_bytes = 5 * 1024^3
)
}
\arguments{
\item{df}{A \code{data.frame} or \code{tibble} containing the data to be written to
Delta Lake.}

\item{target_table}{A character string specifying the name of the Delta
table.
Can be unqualified (\code{"table"}), partially qualified (\code{"schema.table"}), or
fully qualified (\code{"catalog.schema.table"}).
If not fully qualified, the function resolves the catalog and/or schema from
the active database connection.}

\item{db_conn}{A valid DBI connection object to Databricks. This connection
is used to interact with the Delta table.}

\item{column_types_schema}{An optional Arrow Schema object (created via
\code{arrow::schema(...)}) used to explicitly enforce precise data types
during Parquet conversion. If \code{NULL} (default), data types are inferred
from the R data frame.

Type mapping reference (Arrow schema field → Spark SQL type):
\itemize{
\item Arrow int8/int16 → TINYINT/SMALLINT
\item Arrow int32/int64 → INT/BIGINT
\item Arrow float/double → FLOAT/DOUBLE
\item Arrow string/large_string → STRING
\item Arrow bool → BOOLEAN
\item Arrow \code{date32[day]} → DATE
\item Arrow \code{timestamp['s' | 'ms' | 'us', ...]} → TIMESTAMP_NTZ
(no timezone)
\item Arrow \code{timestamp['s' | 'ms' | 'us', tz = ...]} → TIMESTAMP
(with timezone)
\item Arrow decimal(P,S) → DECIMAL(P,S)
}
Important notes for schema use:
\itemize{
\item Factors: If the R data frame contains a \code{factor} column, the
corresponding Arrow schema field must be \code{utf8()} or
\code{large_utf8()}. The categorical labels are automatically converted
and mapped to a \code{STRING} type in the Delta table.
\item Timestamp precision: Second (\code{s}), millisecond (\code{ms}),
and microsecond (\code{us}) precisions are supported. Nanosecond
(\code{ns}) precision may be incompatible with the current Databricks
runtime environment.
}}

\item{volume_dir}{A character string specifying the path to the target
Databricks Volume where the Parquet file will be uploaded.}

\item{copy_options}{A character string specifying options for the
\verb{COPY INTO} command, e.g., \code{'mergeSchema' = 'true'}. Defaults to
\code{"'mergeSchema' = 'true'"}.}

\item{overwrite_table}{Logical; if \code{TRUE}, deletes and recreates the
Delta table before import. If \code{FALSE} and the table does not exist,
the function will throw an error.}

\item{chunk_size_bytes}{An integer specifying the size of each data chunk
in bytes. This is used to split the data frame into smaller chunks for
uploading. Defaults to 5GB.}
}
\value{
Invisibly returns the result of the \verb{COPY INTO} execution.
}
\description{
The function:
\itemize{
\item Optionally overwrites the table
\item Uploads the data as a Parquet file(s) to a Volume
\item Executes a \verb{COPY INTO} command to load the file(s) into Delta Lake
\item Deletes the temporary file(s) after loading
}
}
\details{
This function writes a large R data frame or tibble (\code{df}) to a Delta Lake
table (\code{target_table}) on Databricks using Databricks Volumes and the
\verb{COPY INTO} SQL command.

This function requires the following R packages:
\itemize{
\item \code{DBI}
\item \code{arrow}
\item \code{httr2}
\item \code{dplyr}
}
}
\examples{
\dontrun{
# Setup connection using environment variables
con <- DBI::dbConnect(odbc::databricks(),
  httpPath = Sys.getenv("DATABRICKS_SQL_PATH")
)

write_df_to_delta(
  df = my_data,
  target_table = "catalog.schema.my_table",
  db_conn = con,
  volume_dir = "/Volumes/catalog/schema",
  overwrite_table = TRUE
)
}

}
