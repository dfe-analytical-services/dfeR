<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-GB">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Scalable Bulk Writing to Databricks: write_df_to_delta • dfeR</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Scalable Bulk Writing to Databricks: write_df_to_delta">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">dfeR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.2.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/connecting_to_sql.html">Connecting to SQL</a></li>
    <li><a class="dropdown-item" href="../articles/create_new_project.html">Creating a new dfeR project</a></li>
    <li><a class="dropdown-item" href="../articles/write_df_to_delta.html">Scalable Bulk Writing to Databricks: write_df_to_delta</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/dfe-analytical-services/dfeR/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Scalable Bulk Writing to Databricks: write_df_to_delta</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/dfe-analytical-services/dfeR/blob/main/vignettes/write_df_to_delta.Rmd" class="external-link"><code>vignettes/write_df_to_delta.Rmd</code></a></small>
      <div class="d-none name"><code>write_df_to_delta.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="executive-summary">Executive Summary<a class="anchor" aria-label="anchor" href="#executive-summary"></a>
</h2>
<p>For DfE analysts moving data from RStudio to Databricks via an <a href="https://odbc.r-dbi.org/" class="external-link">odbc</a> connection,
<code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> is a significant step up from <a href="https://dbi.r-dbi.org/" class="external-link">DBI</a> methods. While
<code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code> relies on slow SQL-based uploads,
<code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> uses the <a href="https://docs.databricks.com/api/workspace/introduction" class="external-link">Databricks
REST API</a> to stream data directly into <a href="https://docs.databricks.com/aws/en/volumes" class="external-link">Unity Catalog
Volumes</a>, making the upload faster and more resilient.</p>
<div class="section level3">
<h3 id="why-this-is-a-game-changer-for-your-workflow">Why this is a game-changer for your workflow:<a class="anchor" aria-label="anchor" href="#why-this-is-a-game-changer-for-your-workflow"></a>
</h3>
<ul>
<li>
<strong>Extreme Speed</strong>: <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
moves 1 million rows (42 MB) in 11 seconds—compared to 30 minutes using
<code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code>.</li>
<li>
<strong>Proven at Scale</strong>: Successfully stress-tested up to 1
billion rows (41 GB). To move this volume of data via
<code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code> would take an estimated 20 days of
continuous processing; <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> completes the
task in under half an hour.</li>
<li>
<strong>Bypasses Payload Limits</strong>: Large files often cause
REST API uploads to hang or fail. <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
automatically “chunks” your data, navigating the Databricks REST API
limit of 5 GB per single file upload. As a result, you can upload
gigabytes of data without a hitch.</li>
<li>
<strong>Network Resiliency</strong>: Moving massive files over the
network is prone to transient “blips.” Our built-in auto-retry logic
ensures that if a REST request fails, <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
waits and tries again until the job is done.</li>
<li>
<strong>Schema Safety</strong>: <code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code>
defaults to a 255-character limit for text, that is, it maps strings to
Databricks SQL <code>VARCHAR(255)</code>.
<code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> maps character strings to Databricks
SQL <code>STRING</code> types, meaning your long text fields can be
uploaded without errors.</li>
<li>
<strong>Precise Mapping</strong>: For advanced users,
<code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> allows for refined data type mapping
(e.g., <code>FLOAT</code>, <code>DECIMAL</code>) via Arrow schemas.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="under-the-hood-the-rest-api-advantage">Under the Hood: The REST API Advantage<a class="anchor" aria-label="anchor" href="#under-the-hood-the-rest-api-advantage"></a>
</h2>
<p>Standard DBI uploads send data row-by-row or in SQL batches, which is
incredibly slow for big data. <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> uses a
sequential ingestion strategy:</p>
<ol style="list-style-type: decimal">
<li>It optionally creates or overwrites your Delta Lake table using SQL
(via ODBC).</li>
<li>It converts your data into compressed <a href="https://parquet.apache.org/" class="external-link">Parquet</a> file(s).</li>
<li>It uses the Databricks REST API to upload those temporary file(s) to
a Volume.</li>
<li>It executes a SQL COPY INTO command to merge those file(s) into your
table.</li>
<li>It deletes the temporary file(s) from the Volume.</li>
</ol>
<p><strong>Parquet conversion</strong> is crucial for two reasons:</p>
<ul>
<li>
<strong>Efficiency</strong>: Parquet’s column-oriented compression
significantly reduces the file size, making the network upload much
faster.</li>
<li>
<strong>Compatibility</strong>: Since Delta Lake tables are natively
stored as Parquet files, converting the data upfront ensures a seamless,
high-speed <code>COPY INTO</code> operation later.</li>
</ul>
</div>
<div class="section level2">
<h2 id="prerequisites-permissions-and-authentication">Prerequisites: Permissions and Authentication<a class="anchor" aria-label="anchor" href="#prerequisites-permissions-and-authentication"></a>
</h2>
<p>To ensure a seamless transfer, you must verify that your Databricks
environment and R session are correctly configured.</p>
<div class="section level3">
<h3 id="databricks-permissions">Databricks Permissions<a class="anchor" aria-label="anchor" href="#databricks-permissions"></a>
</h3>
<p>The utility interacts with three different layers of Databricks
security. You will need:</p>
<ul>
<li>
<strong>Catalog &amp; Schema</strong>: <code>USE CATALOG</code> on
the target catalog and <code>USE SCHEMA</code> on the target
schema.</li>
<li>
<strong>Table Management</strong>: <code>CREATE TABLE</code>
permissions on the target schema (required if
<code>overwrite_table = TRUE</code>) or <code>MODIFY</code> and
<code>SELECT</code> permissions on an existing table.</li>
<li>
<strong>Staging Volumes</strong>: <code>READ VOLUME</code> and
<code>WRITE VOLUME</code> on the Unity Catalog Volume used for staging
(specified in the <code>volume_dir</code> argument).</li>
</ul>
</div>
<div class="section level3">
<h3 id="r-session-configuration">R Session Configuration<a class="anchor" aria-label="anchor" href="#r-session-configuration"></a>
</h3>
<p><code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> uses the Databricks REST API for
high-speed data transfer. For the API to authenticate, you must have the
following variables defined in your <code>.Renviron</code> file:</p>
<ul>
<li>
<code>DATABRICKS_HOST</code>: The URL of your workspace.</li>
<li>
<code>DATABRICKS_TOKEN</code>: Your personal access token
(PAT).</li>
</ul>
<p><strong>Tip:</strong> Before you start, use
<code><a href="../reference/check_databricks_odbc.html">check_databricks_odbc()</a></code> to verify that your connection and
environment variables are correctly configured.</p>
</div>
</div>
<div class="section level2">
<h2 id="basic-usage">Basic Usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h2>
<p>Once your permissions are set, using <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
is straightforward. See the example below, or refer to the
<code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> help page for a full description of all
available parameters.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dfe-analytical-services.github.io/dfeR/">dfeR</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dbi.r-dbi.org" class="external-link">DBI</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://odbc.r-dbi.org" class="external-link">odbc</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Establish your connection</span></span>
<span><span class="va">con</span> <span class="op">&lt;-</span> <span class="fu">DBI</span><span class="fu">::</span><span class="fu"><a href="https://dbi.r-dbi.org/reference/dbConnect.html" class="external-link">dbConnect</a></span><span class="op">(</span><span class="fu">odbc</span><span class="fu">::</span><span class="fu">databricks</span><span class="op">(</span><span class="op">)</span>,</span>
<span>                      httpPath <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"DATABRICKS_SQL_PATH"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Upload data frame to Delta Lake</span></span>
<span><span class="co"># The volume_dir is the path to your staging Volume in Unity Catalog</span></span>
<span><span class="fu"><a href="../reference/write_df_to_delta.html">write_df_to_delta</a></span><span class="op">(</span></span>
<span>  df <span class="op">=</span> <span class="va">my_data</span>,</span>
<span>  target_table <span class="op">=</span> <span class="st">"catalog.schema.my_table"</span>,</span>
<span>  db_conn <span class="op">=</span> <span class="va">con</span>,</span>
<span>  volume_dir <span class="op">=</span> <span class="st">"/Volumes/catalog/schema"</span>,</span>
<span>  overwrite_table <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="what-happens-during-execution">What happens during execution?<a class="anchor" aria-label="anchor" href="#what-happens-during-execution"></a>
</h3>
<p>While the function runs, you will see progress updates in the R
console. Because of the auto-retry logic, if the function encounters a
network “blip” during the upload to the Volume or during the final
clean-up/deletion, it will automatically retry the operation, ensuring
your R session remains stable and your Volume stays clean.</p>
<p><strong>Tip:</strong> If you prefer a silent execution, you can wrap
the function in <code><a href="https://rdrr.io/r/base/message.html" class="external-link">suppressMessages()</a></code> to hide the progress
updates.</p>
</div>
</div>
<div class="section level2">
<h2 id="advanced-usage">Advanced Usage<a class="anchor" aria-label="anchor" href="#advanced-usage"></a>
</h2>
<p>While <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> is designed to work “out of
the box,” there are scenarios where you may need precise control over
data types, memory management, or table behaviour.</p>
<div class="section level3">
<h3 id="precise-data-type-mapping-arrow-schemas">Precise Data Type Mapping (Arrow Schemas)<a class="anchor" aria-label="anchor" href="#precise-data-type-mapping-arrow-schemas"></a>
</h3>
<p>If you have specific requirements, such as ensuring a number is a
<code>DECIMAL(18,2)</code> instead of a <code>DOUBLE</code>, you can
pass an <code><a href="https://arrow.apache.org/docs/r/reference/schema.html" class="external-link">arrow::schema()</a></code> directly into the function. This
schema is applied during the Parquet conversion step.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/apache/arrow/" class="external-link">arrow</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">my_custom_schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://arrow.apache.org/docs/r/reference/schema.html" class="external-link">schema</a></span><span class="op">(</span></span>
<span>  transaction_id <span class="op">=</span> <span class="fu"><a href="https://arrow.apache.org/docs/r/reference/data-type.html" class="external-link">int64</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  amount <span class="op">=</span> <span class="fu"><a href="https://arrow.apache.org/docs/r/reference/data-type.html" class="external-link">decimal128</a></span><span class="op">(</span>precision <span class="op">=</span> <span class="fl">18</span>, scale <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/write_df_to_delta.html">write_df_to_delta</a></span><span class="op">(</span></span>
<span>  df <span class="op">=</span> <span class="va">financial_data</span>,</span>
<span>  target_table <span class="op">=</span> <span class="st">"finance.audit.transactions"</span>,</span>
<span>  db_conn <span class="op">=</span> <span class="va">con</span>,</span>
<span>  volume_dir <span class="op">=</span> <span class="st">"/Volumes/main/default/staging/"</span>,</span>
<span>  schema <span class="op">=</span> <span class="va">my_custom_schema</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="manual-chunking-for-memory-management">Manual Chunking for Memory Management<a class="anchor" aria-label="anchor" href="#manual-chunking-for-memory-management"></a>
</h3>
<p>By default, the function handles data in 5 GB chunks. This value is
chosen to safely navigate the Databricks REST API limit while
maintaining high throughput.</p>
<p><strong>Adjusting Chunk Size:</strong> You can use the
<code>chunk_size</code> argument to fine-tune performance based on your
specific dataset:</p>
<ul>
<li>
<strong>Decreasing the size</strong> (e.g., 1 GB): Use this if your
local R session is running out of RAM e.g. during the Parquet conversion
step.</li>
<li>
<strong>Increasing the size</strong>: If you have a very “sparse”
dataset, Parquet compression will be extremely high. In this case, you
could increase the chunk size to process more data per upload since the
resulting Parquet file will be well under the 5 GB REST API limit.</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/write_df_to_delta.html">write_df_to_delta</a></span><span class="op">(</span></span>
<span>  df <span class="op">=</span> <span class="va">my_data</span>,</span>
<span>  target_table <span class="op">=</span> <span class="st">"catalog.schema.my_table"</span>,</span>
<span>  db_conn <span class="op">=</span> <span class="va">con</span>,</span>
<span>  volume_dir <span class="op">=</span> <span class="st">"/Volumes/main/default/staging/"</span>,</span>
<span>  chunk_size <span class="op">=</span> <span class="fl">1</span> <span class="op">*</span> <span class="fl">1024</span><span class="op">^</span><span class="fl">3</span>  <span class="co"># 1 GB in bytes</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p><strong>The Stability vs. Speed Trade-off:</strong> Reducing the
chunk size increases the total number of sequential operations performed
by the function. This adds significant network overhead. We recommend
sticking with the 5 GB default; the function will take longer to finish
with smaller chunks.</p>
</div>
<div class="section level3">
<h3 id="overwriting-vs--appending">Overwriting vs. Appending<a class="anchor" aria-label="anchor" href="#overwriting-vs--appending"></a>
</h3>
<p>By default, <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> will append data to an
existing table. If you want to overwrite the existing table, you must
set <code>overwrite_table = TRUE</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="performance-benchmarks">Performance Benchmarks<a class="anchor" aria-label="anchor" href="#performance-benchmarks"></a>
</h2>
<p>We conducted a series of head-to-head tests of
<code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> against
<code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code>.</p>
<div class="section level3">
<h3 id="methodology">Methodology<a class="anchor" aria-label="anchor" href="#methodology"></a>
</h3>
<p>Tests were performed on the DfE High Memory Desktop (AVD: 137 GB RAM,
16 Cores). Benchmarks utilised a synthetic dataset comprising integers,
numerics, characters, factors, logicals, Dates, and UTC timestamps. This
ensures performance results account for the processing overhead
associated with diverse SQL data types.</p>
<p>To ensure statistical reliability and account for fluctuations in
network traffic or cluster load, we carried out 10 independent runs for
each data volume (rows),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><msup><mn>10</mn><mn>2</mn></msup><mo>,</mo><mi>…</mi><mo>,</mo><msup><mn>10</mn><mn>6</mn></msup><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">n \in \{10^2, \dots, 10^6\}</annotation></semantics></math>.
The benchmarks presented below show the median execution time, with the
error bars representing the interquartile range
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>25</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">25^{th}</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>75</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">75^{th}</annotation></semantics></math>
percentile).</p>
</div>
<div class="section level3">
<h3 id="key-results">Key Results<a class="anchor" aria-label="anchor" href="#key-results"></a>
</h3>
<p>As shown in the graph, for small datasets (&lt; 1,000 rows),
<code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code> is competitive. However, once you
exceed 10,000 rows, the overhead of SQL-based inserts becomes a massive
bottleneck. At 1 million rows, while <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
finishes in roughly 11 seconds, the standard DBI approach takes nearly
30 minutes.</p>
<div class="figure">
<img src="write_df_to_delta_files/figure-html/benchmarks_plot-1.png" class="r-plt" alt="Figure 1: Performance comparison between DBI and dfeR across increasing row counts." width="672"><p class="caption">
Figure 1: Performance comparison between DBI and dfeR across increasing
row counts.
</p>
</div>
</div>
<div class="section level3">
<h3 id="recommendation-choosing-the-right-tool">Recommendation: Choosing the Right Tool<a class="anchor" aria-label="anchor" href="#recommendation-choosing-the-right-tool"></a>
</h3>
<table class="table">
<colgroup>
<col width="33%">
<col width="33%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th align="left">Dataset Size</th>
<th align="left">Recommended Method</th>
<th align="left">Reason</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">
<strong>Small</strong> (&lt; 5k rows)</td>
<td align="left"><code><a href="https://dbi.r-dbi.org/reference/dbWriteTable.html" class="external-link">DBI::dbWriteTable()</a></code></td>
<td align="left">Lower overhead; no need for Volume staging.</td>
</tr>
<tr class="even">
<td align="left">
<strong>Medium</strong> (5k - 100k rows)</td>
<td align="left"><code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code></td>
<td align="left">Significant reduction in execution time compared to
standard SQL-based inserts.</td>
</tr>
<tr class="odd">
<td align="left">
<strong>Large/Stress</strong> (&gt; 1M rows)</td>
<td align="left"><code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code></td>
<td align="left">The only viable method for high-volume transfers within
a standard analytical window.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section level2">
<h2 id="stress-tests">Stress Tests<a class="anchor" aria-label="anchor" href="#stress-tests"></a>
</h2>
<p>To ensure <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> is ready for the DfE’s
largest datasets, we pushed the utility to the practical limit of an R
session’s memory: a synthetic dataset of 1 billion rows (~41 GB). While
the smaller benchmarks focused on pure speed, the stress test evaluated
stability and recovery over long durations.</p>
<div class="section level3">
<h3 id="methodology-1">Methodology<a class="anchor" aria-label="anchor" href="#methodology-1"></a>
</h3>
<p>The stress tests were performed on the DfE High Memory Desktop (AVD).
We used a synthetic dataset (comprising integers, numerics, characters,
factors, logicals, Dates, and UTC timestamps) of up to 1 billion rows
(~41 GB). We conducted 5 independent runs for each data volume (rows),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><msup><mn>10</mn><mn>2</mn></msup><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msup><mn>10</mn><mn>9</mn></msup><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">n \in \{10^2,...,10^9\}</annotation></semantics></math>.</p>
<p>A 1-billion-row dataset represents the practical maximum scale for a
single R session on AVD. While the AVD hardware is robust, a
10-billion-row dataset would exceed R’s in-memory capacity.</p>
<p><strong>Note</strong>: Because these tests were conducted on a shared
DfE cluster, execution times include the real-world impact of concurrent
user activity and network contention.</p>
</div>
<div class="section level3">
<h3 id="the-safety-net-resilience-at-scale">The “Safety Net”: Resilience at Scale<a class="anchor" aria-label="anchor" href="#the-safety-net-resilience-at-scale"></a>
</h3>
<p>When moving bulk data, the biggest risk is the network. Because the
internal functions that handle these uploads are hidden from the user,
it is important to understand the built-in “safety net” that ensures
your data actually arrives:</p>
<ul>
<li><p><strong>Patience for Large Payloads</strong>: We have configured
a 10-minute (600s) “Transfer Safety” window for each chunk. This ensures
that even if a 5GB payload is moving slowly across the network, it isn’t
cut short prematurely.</p></li>
<li><p><strong>Automatic “Self-Healing” Retries</strong>: If an upload
hangs or fails due to a network flicker, the tool triggers an internal
recovery loop. It will automatically attempt to re-upload the specific
failed chunk up to 5 times, waiting for an increasing amount of time
(exponential backoff) between attempts.</p></li>
</ul>
<p><strong>Why this matters</strong>: In the benchmarks, you may notice
variability in execution times. This usually indicates the tool detected
a network issue, waited for the safety window, and successfully retried
the upload automatically.</p>
</div>
<div class="section level3">
<h3 id="key-results-and-observations">Key Results and Observations<a class="anchor" aria-label="anchor" href="#key-results-and-observations"></a>
</h3>
<p>The boxplot below captures both the speed and the stability of the
transfer across eight orders of magnitude.</p>
<ul>
<li><p><strong>Linear Scaling</strong>: <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
demonstrates consistent, predictable scaling even as it approaches the
practical memory ceiling.</p></li>
<li><p><strong>Automated Chunk Management</strong>: The Databricks REST
API has a strict 5 GB limit per file. <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code>
automatically calculated and managed the 41 GB upload in 9 sequential
chunks. This removes the need for analysts to manually slice their data,
ensuring each piece stays under the API limit while maintaining maximum
throughput.</p></li>
<li><p><strong>Stability &amp; Memory Efficiency</strong>: Despite the
massive data volume, the R session maintained a stable ~5 GB overhead.
By streaming data in chunks, the utility avoids “double-buffering” the
data in RAM, which prevents the RStudio crashes that may happen during
large-scale exports.</p></li>
<li><p><strong>Reliability Trade-off</strong>: Sequential chunking
introduces a small amount of network overhead for each “slice.” While
this makes the total execution time longer than a single (theoretical)
massive upload, it is a necessary trade-off to ensure the transfer is
fault-tolerant and bypasses REST API payload limits.</p></li>
</ul>
<div class="figure">
<img src="write_df_to_delta_files/figure-html/stress_test_plot-1.png" class="r-plt" alt="Figure 2: Performance resiliency testing from 100 to 1 billion rows, showing stable execution times." width="672"><p class="caption">
Figure 2: Performance resiliency testing from 100 to 1 billion rows,
showing stable execution times.
</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="troubleshooting">Troubleshooting<a class="anchor" aria-label="anchor" href="#troubleshooting"></a>
</h2>
<p>The <code><a href="../reference/write_df_to_delta.html">write_df_to_delta()</a></code> function includes extensive
validation checks. In most cases, if an error occurs, the console will
provide a specific, descriptive message detailing the issue. <strong>If
you encounter an error, please read the console output
first</strong>.</p>
<ul>
<li>
<strong>Data Type &amp; Schema Mismatches</strong>: If you are
appending to a table, you must ensure that your R data types are
compatible with the existing Delta table schema. Even when overwriting
the table, conflicts can arise during the three-layer mapping process:
the R class, the Apache Arrow schema (used for Parquet conversion), and
the Databricks SQL type.
<ul>
<li>If you hit a mismatch, try explicitly casting your R columns before
running the function to ensure the Arrow conversion maps correctly to
the target SQL types.</li>
<li>Use the <code>column_types_schema</code> argument to enforce precise
types (e.g., <code>arrow::timestamp("us")</code> or
<code><a href="https://arrow.apache.org/docs/r/reference/data-type.html" class="external-link">arrow::utf8()</a></code> for factors). This ensures the Parquet file
strictly matches the expectations of the Databricks
<code>COPY INTO</code> command. See <a href="#precise-data-type-mapping-arrow-schemas">Precise Data Type
Mapping (Arrow Schemas)</a>.</li>
<li>Pay close attention to decimal scales and timestamp precisions
(seconds, milliseconds, or microseconds), as these must align with the
target SQL column to avoid ingestion failures. Avoid nanosecond
(<code>ns</code>) precision, as it is often incompatible with Databricks
runtimes.</li>
</ul>
</li>
<li>
<strong>Permission &amp; Authentication Errors</strong>: If your
message mentions missing access or failed handshakes, ensure your
<code>.Renviron</code> is configured correctly and that you have the
necessary permissions to the target catalog, schema and Volume; see <a href="#prerequisites-permissions-and-authentication">Permissions and
Authentication</a>.</li>
<li>
<strong>Network Timeouts or “Hangs”</strong>: If the progress bar
appears to stall, the utility is likely managing a network flicker using
its internal retry loop. You can read about this in <a href="#the-safety-net-resilience-at-scale">The “Safety Net”: Resilience
at Scale</a>.</li>
<li>
<strong>Performance Variability</strong>: Fluctuations in execution
time are typically due to shared cluster load or network traffic on the
DfE environment. Variability can also occur when the function triggers
internal retries to recover from transient network failures.</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Cam Race, Laura Selby, Adam Robinson.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
